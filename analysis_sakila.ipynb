{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install necessary libraries\n",
    "!pip install google-generativeai langchain-community transformers accelerate tiktoken faiss-cpu\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import faiss\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import git\n",
    "\n",
    "# Step 1: Clone the SakilaProject GitHub repository\n",
    "repo_url = \"https://github.com/janjakovacevic/SakilaProject.git\"\n",
    "repo_dir = '/content/SakilaProject'\n",
    "\n",
    "# Clone the repository if it does not exist\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(\"Cloning SakilaProject repository...\")\n",
    "    git.Repo.clone_from(repo_url, repo_dir)\n",
    "else:\n",
    "    print(\"SakilaProject repository already exists!\")\n",
    "\n",
    "# Step 2: Setup Gemini API\n",
    "genai.configure(api_key=\"AIzaSyBi6lAb1umq0z_LT1n8TW92QAfulacux3U\")\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# Step 3: Read Java files from the cloned repository\n",
    "print(\"Files detected:\")\n",
    "\n",
    "all_code = \"\"\n",
    "for root, dirs, files in os.walk(repo_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".java\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Found Java file: {file_path}\")\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                if content.strip():\n",
    "                    all_code += f\"\\n\\n// File: {file}\\n\\n\" + content\n",
    "\n",
    "print(f\"\\nTotal characters of Java code read: {len(all_code)}\")\n",
    "\n",
    "# Step 4: Chunk the code into smaller pieces\n",
    "chunk_size = 800\n",
    "chunks = [all_code[i:i+chunk_size] for i in range(0, len(all_code), chunk_size)]\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "\n",
    "# Step 5: Load local HuggingFace model and embedding model\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\", max_length=1024)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 6: Generate embeddings\n",
    "all_embeddings = []\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"Generating embedding for chunk {idx + 1}/{len(chunks)}...\")\n",
    "    embeddings = embedding_model.embed_documents([chunk])\n",
    "    all_embeddings.append(np.array(embeddings))\n",
    "\n",
    "# Stack all embeddings into a numpy array\n",
    "all_embeddings_np = np.vstack(all_embeddings)\n",
    "embedding_dim = all_embeddings_np.shape[-1]  # This is the dimension of the embeddings\n",
    "\n",
    "# Step 7: Create and train a FAISS index\n",
    "\n",
    "nlist = 100  # Number of Voronoi cells (clusters)\n",
    "\n",
    "# Initialize the quantizer index (IndexFlatL2)\n",
    "quantizer = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Initialize the FAISS index with the quantizer\n",
    "faiss_index = faiss.IndexIVFFlat(quantizer, embedding_dim, nlist, faiss.METRIC_L2)\n",
    "\n",
    "# Train the FAISS index with the embeddings\n",
    "faiss_index.train(all_embeddings_np)\n",
    "\n",
    "# Add the embeddings to the FAISS index\n",
    "faiss_index.add(all_embeddings_np)\n",
    "\n",
    "# Step 8: Save FAISS index\n",
    "faiss.write_index(faiss_index, '/content/sakila_faiss.index')\n",
    "print(f\"‚úÖ FAISS index saved at /content/sakila_faiss.index\")\n",
    "\n",
    "# Step 9: Query function\n",
    "def query_faiss_index(query_text, top_k=3):\n",
    "    query_embedding = embedding_model.embed_documents([query_text])\n",
    "    query_embedding_np = np.array(query_embedding[0]).reshape(1, -1)\n",
    "    _, indices = faiss_index.search(query_embedding_np, top_k)\n",
    "    top_chunks = [chunks[idx] for idx in indices[0]]\n",
    "    return top_chunks\n",
    "\n",
    "# Example query\n",
    "query = \"How to create a new user in the database?\"\n",
    "top_chunks = query_faiss_index(query)\n",
    "print(\"\\nTop matching code chunks:\")\n",
    "for idx, chunk in enumerate(top_chunks):\n",
    "    print(f\"\\n--- Chunk {idx + 1} ---\\n{chunk[:500]}...\")  # show first 500 chars\n",
    "\n",
    "# Step 10: Analyze each chunk using Gemini model\n",
    "overview_texts = []\n",
    "functions_list = []\n",
    "complexity_scores = []\n",
    "complexity_descriptions = []\n",
    "\n",
    "# Helper to extract JSON from Gemini output\n",
    "def extract_json(text):\n",
    "    try:\n",
    "        json_text = re.search(r\"\\{.*\\}\", text, re.DOTALL).group()\n",
    "        return json.loads(json_text)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to extract JSON: {e}\")\n",
    "        print(\"Model output was:\\n\", text)\n",
    "        return None\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"Analyzing chunk {idx + 1}/{len(chunks)}...\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert codebase analyzer.\n",
    "Analyze the following Java code and extract:\n",
    "- A high-level overview (2-3 sentences)\n",
    "- List of key functions (function signature + description)\n",
    "- Code complexity (score out of 10 + reason)\n",
    "Return ONLY a JSON like this:\n",
    "{{\n",
    "  \"project overview\": \"#####\",\n",
    "  \"functions\": [\n",
    "    {{\"function name\": \"#####\", \"description\": \"#####\"}},\n",
    "    ...\n",
    "  ],\n",
    "  \"complexity\": {{\"score\": \"#/10\", \"description\": \"#####\"}}\n",
    "}}\n",
    "\n",
    "Code:\n",
    "{chunk}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        output = response.text.strip()\n",
    "\n",
    "        parsed = extract_json(output)\n",
    "\n",
    "        if parsed:\n",
    "            overview_texts.append(parsed.get(\"project overview\", \"\"))\n",
    "\n",
    "            functions = parsed.get(\"functions\", [])\n",
    "            if isinstance(functions, list):\n",
    "                functions_list.extend(functions)\n",
    "\n",
    "            complexity = parsed.get(\"complexity\", {})\n",
    "            if complexity:\n",
    "                complexity_score = complexity.get(\"score\", \"5/10\")\n",
    "                complexity_description = complexity.get(\"description\", \"Not provided\")\n",
    "\n",
    "                # Extract score as integer\n",
    "                if isinstance(complexity_score, str) and \"/\" in complexity_score:\n",
    "                    complexity_score = int(complexity_score.split(\"/\")[0])\n",
    "                elif isinstance(complexity_score, (int, float)):\n",
    "                    complexity_score = int(complexity_score)\n",
    "\n",
    "                complexity_scores.append(complexity_score)\n",
    "                complexity_descriptions.append(complexity_description)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing chunk {idx}: {e}\")\n",
    "\n",
    "# Step 11: Merge results properly\n",
    "average_score = round(sum(complexity_scores) / max(len(complexity_scores), 1))\n",
    "\n",
    "final_json = {\n",
    "    \"project overview\": \" \".join(overview_texts),\n",
    "    \"functions\": functions_list,\n",
    "    \"complexity\": {\n",
    "        \"score\": f\"{average_score}/10\",\n",
    "        \"description\": \" \".join(complexity_descriptions)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 12: Save final JSON\n",
    "output_path = '/content/sakila_final_analysis2.json'\n",
    "if final_json:\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_json, f, indent=4)\n",
    "    print(f\"‚úÖ Final structured JSON saved at {output_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No final analysis generated!\")\n",
    "\n",
    "# Step 13: Display the final result\n",
    "import pprint\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"\\nüìÑ Final Project Analysis JSON:\")\n",
    "pprint.pprint(data)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
